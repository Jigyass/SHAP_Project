{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99c7c612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%matplotlib inline\\nimport torch\\nfrom torch.utils.data import DataLoader, Subset\\nfrom torchvision import datasets, transforms, models\\nfrom torchvision.models import VGG16_Weights\\nimport shap\\nimport random\\nimport matplotlib.pyplot as plt\\nimport time\\nimport matplotlib\\nimport numpy as np\\n\\n# Start the timer\\nstart_time = time.time()\\nprint(\"Script started.\")\\n\\n# Path to your images\\nimage_path = \\'/home/jigyas/Github/SHAP_Project/flickr30k_images/flickr30k_images\\'\\n\\n# Image transformations\\ntransform = transforms.Compose([\\n    transforms.Resize(256),\\n    transforms.CenterCrop(224),\\n    transforms.ToTensor(),\\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n])\\n\\nprint(\"Loading the dataset...\")\\n# Load the dataset\\ndataset = datasets.ImageFolder(image_path, transform=transform)\\n\\n# Selecting a subset of x images\\nsubset_size = 1\\nsubset_indices = random.sample(range(len(dataset)), subset_size)\\nsubset_dataset = Subset(dataset, subset_indices)\\n\\n# DataLoader for the subset\\nbatch_size = 32\\nsubset_data_loader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True)\\n\\ndef replace_inplace_relu_with_standard(model):\\n    for name, module in model.named_children():\\n        if isinstance(module, torch.nn.ReLU):\\n            setattr(model, name, torch.nn.ReLU(inplace=False))\\n        elif len(list(module.children())) > 0:\\n            replace_inplace_relu_with_standard(module)\\n\\nprint(\"Loading and modifying the VGG16 model...\")\\n# Load the VGG16 model\\nmodel = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\\n\\n# Replace in-place ReLU with standard ReLU\\nreplace_inplace_relu_with_standard(model)\\n\\n# Ensure model is in evaluation mode\\nmodel.eval()\\n\\n# Check if CUDA is available and use GPU if possible\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel.to(device)\\n\\n# Function for batch prediction\\ndef batch_predict(images):\\n    images = images.to(device)\\n    with torch.no_grad():\\n        outputs = model(images)\\n    return torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()\\n\\nprint(\"Starting SHAP analysis...\")\\n# Get a single batch of data\\nsample_images, _ = next(iter(subset_data_loader))\\n\\n# Initialize SHAP DeepExplainer\\nexplainer = shap.DeepExplainer(model, sample_images.to(device))\\n\\n# Calculate SHAP values\\nshap_values = explainer.shap_values(sample_images.to(device))\\n\\nplt.figure(figsize=(20, 10))\\n\\n# Visualize the SHAP values for the first few predictions\\nshap.image_plot(shap_values, sample_images.numpy())\\n\\n# End the timer and calculate the total time taken\\nend_time = time.time()\\ntotal_time = end_time - start_time\\nprint(f\"SHAP analysis completed. Total time taken: {total_time:.2f} seconds\")\\n\\nplt.show()\\n\\n# Ensure that there is a batch of images and SHAP values to visualize\\nif len(sample_images) > 0 and len(shap_values) > 0:\\n    plt.figure(figsize=(20, 15))  # Increase the size of the figure\\n\\n    for i in range(min(len(sample_images), 5)):  # Adjust the number of images you want to display\\n        plt.subplot(2, min(len(sample_images), 5), i + 1)\\n        plt.imshow(sample_images[i].numpy().transpose(1, 2, 0))\\n        plt.axis(\\'off\\')\\n\\n        plt.subplot(2, min(len(sample_images), 5), i + 1 + min(len(sample_images), 5))\\n        plt.imshow(shap_values[0][i].transpose(1, 2, 0))\\n        plt.axis(\\'off\\')\\n\\n    plt.show()\\nelse:\\n    print(\"No data to display.\") '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%matplotlib inline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import VGG16_Weights\n",
    "import shap\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "print(\"Script started.\")\n",
    "\n",
    "# Path to your images\n",
    "image_path = '/home/jigyas/Github/SHAP_Project/flickr30k_images/flickr30k_images'\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Loading the dataset...\")\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(image_path, transform=transform)\n",
    "\n",
    "# Selecting a subset of x images\n",
    "subset_size = 1\n",
    "subset_indices = random.sample(range(len(dataset)), subset_size)\n",
    "subset_dataset = Subset(dataset, subset_indices)\n",
    "\n",
    "# DataLoader for the subset\n",
    "batch_size = 32\n",
    "subset_data_loader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def replace_inplace_relu_with_standard(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.ReLU):\n",
    "            setattr(model, name, torch.nn.ReLU(inplace=False))\n",
    "        elif len(list(module.children())) > 0:\n",
    "            replace_inplace_relu_with_standard(module)\n",
    "\n",
    "print(\"Loading and modifying the VGG16 model...\")\n",
    "# Load the VGG16 model\n",
    "model = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Replace in-place ReLU with standard ReLU\n",
    "replace_inplace_relu_with_standard(model)\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Check if CUDA is available and use GPU if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function for batch prediction\n",
    "def batch_predict(images):\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    return torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "print(\"Starting SHAP analysis...\")\n",
    "# Get a single batch of data\n",
    "sample_images, _ = next(iter(subset_data_loader))\n",
    "\n",
    "# Initialize SHAP DeepExplainer\n",
    "explainer = shap.DeepExplainer(model, sample_images.to(device))\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(sample_images.to(device))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Visualize the SHAP values for the first few predictions\n",
    "shap.image_plot(shap_values, sample_images.numpy())\n",
    "\n",
    "# End the timer and calculate the total time taken\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"SHAP analysis completed. Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Ensure that there is a batch of images and SHAP values to visualize\n",
    "if len(sample_images) > 0 and len(shap_values) > 0:\n",
    "    plt.figure(figsize=(20, 15))  # Increase the size of the figure\n",
    "\n",
    "    for i in range(min(len(sample_images), 5)):  # Adjust the number of images you want to display\n",
    "        plt.subplot(2, min(len(sample_images), 5), i + 1)\n",
    "        plt.imshow(sample_images[i].numpy().transpose(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(2, min(len(sample_images), 5), i + 1 + min(len(sample_images), 5))\n",
    "        plt.imshow(shap_values[0][i].transpose(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data to display.\") '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0512328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started.\n",
      "Loading the dataset...\n",
      "Loading the VGG16 model...\n",
      "Starting SHAP analysis...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_416/1961668488.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Calculate SHAP values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Visualize the SHAP values for the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_deep/__init__.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mwere\u001b[0m \u001b[0mchosen\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m\"top\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranked_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_rank_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_additivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_additivity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_deep/deep_pytorch.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;31m# run attribution computation graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mfeature_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output_ranks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 \u001b[0msample_phis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# assign the attributions to the right part of the output arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_deep/deep_pytorch.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, idx, inputs)\u001b[0m\n\u001b[1;32m    127\u001b[0m                                            allow_unused=True)[0]\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import VGG16_Weights\n",
    "import shap\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "print(\"Script started.\")\n",
    "\n",
    "# Path to your images\n",
    "image_path = '/home/jigyas/Github/SHAP_Project/fruits-360_dataset/fruits-360/Training'\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Loading the dataset...\")\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(image_path, transform=transform)\n",
    "\n",
    "# Selecting a subset of 2 images for quicker analysis\n",
    "subset_size = 1\n",
    "subset_indices = random.sample(range(len(dataset)), subset_size)\n",
    "subset_dataset = Subset(dataset, subset_indices)\n",
    "\n",
    "# DataLoader for the subset with multiple workers for parallel image loading\n",
    "batch_size = subset_size  # Setting batch size equal to subset size for one-shot loading\n",
    "num_workers = 4  # Adjust this based on your system's CPU cores\n",
    "subset_data_loader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading the VGG16 model...\")\n",
    "# Load the VGG16 model\n",
    "model = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "model.eval()\n",
    "\n",
    "# Check if CUDA is available and use GPU if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def replace_inplace_relu_with_standard(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.ReLU) and module.inplace:\n",
    "            setattr(model, name, torch.nn.ReLU(inplace=False))\n",
    "        else:\n",
    "            for child_name, child_module in module.named_children():\n",
    "                if isinstance(child_module, torch.nn.ReLU) and child_module.inplace:\n",
    "                    setattr(module, child_name, torch.nn.ReLU(inplace=False))\n",
    "                replace_inplace_relu_with_standard(child_module)\n",
    "\n",
    "# Apply the replacement function to the model\n",
    "replace_inplace_relu_with_standard(model)\n",
    "\n",
    "# Function for batch prediction that will be used by SHAP\n",
    "def batch_predict(images):\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    return torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "print(\"Starting SHAP analysis...\")\n",
    "# Get a single batch of data\n",
    "sample_images, _ = next(iter(subset_data_loader))\n",
    "\n",
    "# Initialize SHAP DeepExplainer\n",
    "explainer = shap.DeepExplainer(model, sample_images.to(device))\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(sample_images.to(device))\n",
    "\n",
    "# Visualize the SHAP values for the predictions\n",
    "shap.image_plot(shap_values, sample_images.numpy())\n",
    "\n",
    "# End the timer and calculate the total time taken\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"SHAP analysis completed. Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9909870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 18:10:56.937133: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-14 18:10:56.937185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-14 18:10:56.968755: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-14 18:10:57.049345: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-14 18:10:57.835472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-12-14 18:10:58.545634: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-14 18:10:58.550026: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-14 18:10:58.550047: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-14 18:10:58.556496: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-14 18:10:58.556528: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-14 18:10:58.556540: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-14 18:10:58.565423: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-14 18:10:58.565458: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-14 18:10:58.565464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-12-14 18:10:58.565498: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-14 18:10:58.565519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0a:00.0, compute capability: 8.6\n",
      "2023-12-14 18:11:08.587221: W external/local_tsl/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.0KiB (rounded to 1280)requested by op ScratchBuffer\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-12-14 18:11:08.587244: I external/local_tsl/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-12-14 18:11:08.587251: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587254: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587257: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587260: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587263: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587265: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587272: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587275: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587278: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587281: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587283: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587286: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587289: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587291: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587294: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587297: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587299: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587302: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587305: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587308: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587310: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-12-14 18:11:08.587314: I external/local_tsl/tsl/framework/bfc_allocator.cc:1062] Bin for 1.2KiB was 1.0KiB, Chunk State: \n",
      "2023-12-14 18:11:08.587316: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-12-14 18:11:08.587319: I external/local_tsl/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 0B\n",
      "2023-12-14 18:11:08.587322: I external/local_tsl/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 0 memory_limit_: 0 available bytes: 0 curr_region_allocation_bytes_: 0\n",
      "2023-12-14 18:11:08.587328: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                               0\n",
      "InUse:                               0\n",
      "MaxInUse:                            0\n",
      "NumAllocs:                           0\n",
      "MaxAllocSize:                        0\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-12-14 18:11:08.587366: W external/local_tsl/tsl/framework/bfc_allocator.cc:497] <allocator contains no memory>\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "{{function_node __wrapped____EagerConst_device_/job:localhost/replica:0/task:0/device:GPU:0}} Failed to allocate scratch buffer for device 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_416/1185425006.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load the VGG16 model pre-trained on ImageNet data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mvgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Freeze the layers of VGG16 model since they are already trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/applications/vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mimg_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m# Block 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     x = layers.Conv2D(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"block1_conv1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     )(img_input)\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2099\u001b[0m                 \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateless_fold_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2100\u001b[0;31m             return tf.random.stateless_uniform(\n\u001b[0m\u001b[1;32m   2101\u001b[0m                 \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m                 \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: {{function_node __wrapped____EagerConst_device_/job:localhost/replica:0/task:0/device:GPU:0}} Failed to allocate scratch buffer for device 0"
     ]
    }
   ],
   "source": [
    "'''from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Define image size and path to the dataset\n",
    "IMAGE_SIZE = [64, 64]  # keep the image size as (64, 64)\n",
    "training_dir = '/home/jigyas/Github/SHAP_Project/fruits-360_dataset/fruits-360/Training'\n",
    "validation_dir = '/home/jigyas/Github/SHAP_Project/fruits-360_dataset/fruits-360/Test'\n",
    "\n",
    "# Load the VGG16 model pre-trained on ImageNet data\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "\n",
    "# Freeze the layers of VGG16 model since they are already trained\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of VGG16\n",
    "x = Flatten()(vgg.output)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)  # assuming num_classes is the number of fruit classes\n",
    "\n",
    "# Create the full model\n",
    "model = Model(inputs=vgg.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Create image data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "# Prepare data loaders\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,  # This should be the number of batches per epoch (total images/batch size)\n",
    "    epochs=10,           # Number of epochs to train for\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=50  # This should be number of batches in validation set\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041b0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
